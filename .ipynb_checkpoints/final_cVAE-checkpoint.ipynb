{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for cVAE\n",
    "#### By Ella Johnsen and Eivind Lysheim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.layers import Activation, Dense, Input, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Reshape, Concatenate, Lambda, Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error, binary_crossentropy\n",
    "from keras import metrics\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "OLD_IMAGE_DIMS = (218, 178, 3)\n",
    "NEW_IMAGE_DIMS = (64, 64, 3)\n",
    "BATCH_SIZE = 32\n",
    "N = 10000\n",
    "NUM_ATTRIBUTES = 40\n",
    "NUM_BATCHES = 200\n",
    "LATENT_DIM = 64\n",
    "TOT_IMAGES = 202599\n",
    "EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the paths to the dataset. Only use one of these\n",
    "\n",
    "# Paths for Kaggle\n",
    "PATH = '../input/celeba-dataset/'\n",
    "IMAGES = \"img_align_celeba/img_align_celeba/\"\n",
    "ATTRIBUTES = \"../input/list-attr-celebatxt/list_attr_celeba.txt\"\n",
    "\n",
    "# Paths for running locally\n",
    "#PATH = './'\n",
    "#IMAGES = 'img_align_celeba/'\n",
    "#ATTRIBUTES = 'list_attr_celeba.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the attributes from file\n",
    "def get_attributes(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    attributes = []\n",
    "    attr = []\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        if i != 0 and i != 1:\n",
    "            line = line.split()\n",
    "            attributes.append(line)\n",
    "        i += 1\n",
    "    f.close()\n",
    "    return attributes\n",
    "\n",
    "# Collecting the names of all 40 possible attributes\n",
    "def get_attribute_names(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    return lines[1].split()\n",
    "\n",
    "# Function that makes a training set\n",
    "def initialize_training_set(): \n",
    "    all_attributes = get_attributes(ATTRIBUTES)\n",
    "    \n",
    "    while True:\n",
    "        chosen_info = random.sample(all_attributes, BATCH_SIZE)#N)\n",
    "        chosen_attributes = [info[1:] for info in chosen_info]\n",
    "        chosen_images = [info[0] for info in chosen_info]\n",
    "        resized_images = []\n",
    "    \n",
    "        for ind in range(len(chosen_info)):\n",
    "            image_path = PATH + IMAGES + chosen_images[ind]\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_COLOR)[...,::-1] / 255.0\n",
    "            img = img[45:173,25:153]\n",
    "            img = cv2.resize(src=img, dsize=(64, 64))\n",
    "            resized_images.append(img)\n",
    "\n",
    "        resized_images = np.array(resized_images, dtype=np.float32)\n",
    "        chosen_attributes = np.array(chosen_attributes, dtype=np.float32)\n",
    "\n",
    "        yield [resized_images, chosen_attributes], None \n",
    "        \n",
    "# Function that makes a test set\n",
    "def initialize_test_set():\n",
    "    all_attributes = get_attributes(ATTRIBUTES)\n",
    "    info_in_batches = []\n",
    "    attributes_in_batches = []\n",
    "    images_in_batches = []\n",
    "    test_set = []\n",
    "    \n",
    "    for i in range(NUM_BATCHES):\n",
    "        chosen_info = random.sample(all_attributes, BATCH_SIZE)\n",
    "        chosen_attributes = [info[1:] for info in chosen_info]\n",
    "        chosen_images = [info[0] for info in chosen_info]\n",
    "        \n",
    "        info_in_batches.append(chosen_info)\n",
    "        attributes_in_batches.append(chosen_attributes)\n",
    "        images_in_batches.append(chosen_images)\n",
    "        \n",
    "    for i in range(NUM_BATCHES):\n",
    "        chosen_images = images_in_batches[i]\n",
    "        attributes = attributes_in_batches[i]\n",
    "        resized_images = []\n",
    "        \n",
    "        for j in range(BATCH_SIZE):\n",
    "            image_path = PATH + IMAGES + chosen_images[j]\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_COLOR)[...,::-1] / 255.0\n",
    "            img = img[45:173,25:153]\n",
    "            img = cv2.resize(src=img, dsize=(64, 64))\n",
    "            resized_images.append(img)\n",
    "\n",
    "        resized_images = np.array(resized_images, dtype=np.float32)\n",
    "        attributes = np.array(chosen_attributes, dtype=np.float32)\n",
    "\n",
    "        test_set.append([resized_images, attributes])\n",
    "    return test_set\n",
    "\n",
    "# Initializing a training set and a test set\n",
    "training_set = initialize_training_set()\n",
    "test_set = initialize_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the cVAE model\n",
    "\n",
    "class Sampling(Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def VAE():\n",
    "    # Encoding\n",
    "    input_img = Input(shape = NEW_IMAGE_DIMS, name='input_img')\n",
    "    labels = Input(shape = (NUM_ATTRIBUTES,), name='labels')\n",
    "\n",
    "    x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(input_img)\n",
    "    x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "    x = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "    x = Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "    x = Conv2D(filters = 512, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "\n",
    "    shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    z_mean = Dense(LATENT_DIM, name='z_mean')(x)\n",
    "    z_log_sigma = Dense(LATENT_DIM, name='z_log_sigma')(x)\n",
    "    z = Sampling()([z_mean, z_log_sigma])\n",
    "\n",
    "    zy = Concatenate()([z, labels])\n",
    "\n",
    "    inputs_embedding = Input(shape=(LATENT_DIM + NUM_ATTRIBUTES,))\n",
    "    embedding = Dense(np.prod(shape_before_flattening))(inputs_embedding)\n",
    "    embedding = Reshape(shape_before_flattening)(embedding)\n",
    "\n",
    "    #Decoding\n",
    "    x_ = Conv2DTranspose(filters = 256, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(embedding)\n",
    "    x_ = Conv2DTranspose(filters = 128, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "    x_ = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "    x_ = Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "    x_ = Conv2DTranspose(filters = 3, kernel_size = 3, strides = 2,  padding = 'same', activation = 'sigmoid')(x_)\n",
    "\n",
    "    encoder = Model(inputs = [input_img, labels], outputs = zy, name=\"encoder\")\n",
    "    decoder = Model(inputs = inputs_embedding, outputs = x_, name=\"decoder\")\n",
    "\n",
    "    vae_out = decoder(encoder([input_img, labels]))\n",
    "\n",
    "    rec_loss =  np.prod(NEW_IMAGE_DIMS) * binary_crossentropy(Flatten()(input_img), Flatten()(vae_out))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    vae_loss = K.mean(rec_loss + kl_loss)\n",
    "\n",
    "    vae = Model(inputs = [input_img, labels], outputs = vae_out, name=\"vae\")\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "\n",
    "    optimizer = Adam(lr=0.0005, beta_1 = 0.5)\n",
    "    vae.compile(optimizer)\n",
    "\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "vae, encoder, decoder = VAE()\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reconstruct given images\n",
    "def reconstruct_images(test_set):\n",
    "    print('Reconstructing images')\n",
    "    images_for_fid = []\n",
    "    predictions_for_fid = []\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    for batch in test_set:\n",
    "        print('Batch:', index)\n",
    "        example_images = batch[0] * 255.0\n",
    "        example_predictions = vae.predict(batch) * 255.0\n",
    "        \n",
    "        # Scaling images\n",
    "        scaled_images = scale_images(example_images, (299, 299, 3))\n",
    "        scaled_predictions = scale_images(example_predictions, (299, 299, 3))\n",
    "        \n",
    "        # Pre-processing images\n",
    "        preprocessed_images = preprocess_input(scaled_images)\n",
    "        preprocessed_predictions = preprocess_input(scaled_predictions)\n",
    "        \n",
    "        act1 = model.predict(preprocessed_images)\n",
    "        act2 = model.predict(preprocessed_predictions)\n",
    "        \n",
    "        images_for_fid.append(act1)\n",
    "        predictions_for_fid.append(act2)\n",
    "                      \n",
    "        index += 1\n",
    "\n",
    "    return example_images, example_predictions, images_for_fid, predictions_for_fid\n",
    "\n",
    "# Functions to generate new images given user-specified attributes\n",
    "def generate_images(generated_attributes):\n",
    "    print('')\n",
    "    print('Generating images')\n",
    "    \n",
    "    predictions_for_fid = []\n",
    "    \n",
    "    images = np.random.uniform(-1, 1, size = (NUM_BATCHES * BATCH_SIZE, LATENT_DIM)).astype(np.float32)\n",
    "    \n",
    "    for i in range(NUM_BATCHES):\n",
    "        print('Batch:', i+1)\n",
    "        batch = []\n",
    "        \n",
    "        for j in range(BATCH_SIZE):\n",
    "            img_info = []\n",
    "            img_info.append(np.concatenate((images[j].flatten(), generated_attributes)))\n",
    "            batch.append(np.array(img_info).flatten())\n",
    "            \n",
    "        batch = np.array(batch)\n",
    "        predictions = decoder.predict(batch) * 255.0\n",
    "        \n",
    "        # Scaling images\n",
    "        scaled_predictions = scale_images(predictions, (299, 299, 3))\n",
    "        \n",
    "        # Pre-processing images\n",
    "        preprocessed_predictions = preprocess_input(scaled_predictions)\n",
    "        \n",
    "        \n",
    "        #Making activations\n",
    "        act = model.predict(preprocessed_predictions)\n",
    "        \n",
    "        predictions_for_fid.append(act)\n",
    "\n",
    "    return predictions, predictions_for_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Frechet Inception Distance (https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/)\n",
    "\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return np.asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(act1, act2):\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_images(batch):\n",
    "    used_indices = []\n",
    "    nrow = 2\n",
    "    ncol = 8\n",
    "    fig = plt.figure(figsize=((ncol+1)*1.5, (nrow+1)*1.5)) \n",
    "    gs = mpl.gridspec.GridSpec(nrow, ncol,\n",
    "             wspace=0.0, hspace=0.0, \n",
    "             top=1.-0.5/(nrow+1), bottom=0.5/(nrow+1), \n",
    "             left=0.5/(ncol+1), right=1-0.5/(ncol+1)) \n",
    "    for i in range(16):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        plt.imshow(batch[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "vae.fit(training_set, steps_per_epoch = N//BATCH_SIZE, verbose = 1, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "vae.save_weights('./vae.h5')\n",
    "decoder.save_weights('./decoder.h5')\n",
    "encoder.save_weights('./encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the inception v3 model\n",
    "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing images with VAE\n",
    "rec_images, rec_predictions, images_for_fid, rec_predictions_for_fid = reconstruct_images(test_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original images')\n",
    "show_images(rec_images/255.0)\n",
    "print('\\n')\n",
    "print('Reconstructed images')\n",
    "show_images(rec_predictions/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing all possible attributes user can pick from and their index\n",
    "attributes = get_attribute_names(ATTRIBUTES)\n",
    "\n",
    "for i in range(len(attributes)):\n",
    "    print(i, attributes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking user-specified attributes\n",
    "\n",
    "# Initializing attributes\n",
    "chosen_attributes = np.ones(NUM_ATTRIBUTES, dtype=np.float32) * -1\n",
    "\n",
    "# Example 1\n",
    "#chosen_attributes[4] = 1  # bald\n",
    "#chosen_attributes[22] = 1 # mustache\n",
    "\n",
    "# Example 2\n",
    "#chosen_attributes[8] = 1   # black hair\n",
    "#chosen_attributes[20] = 1  # male\n",
    "#chosen_attributes[31] = 1  # smiling\n",
    "\n",
    "# Example 3\n",
    "chosen_attributes[13] = 1  # chubby\n",
    "chosen_attributes[18] = 1 # heavy makeup\n",
    "chosen_attributes[32] = 1  # straight hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new images with VAE\n",
    "gen_predictions, gen_predictions_for_fid = generate_images(chosen_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
