{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.layers import Activation, Dense, Input, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Reshape, Concatenate, Lambda, Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error, binary_crossentropy\n",
    "from keras import metrics\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "OLD_IMAGE_DIMS = (218, 178, 3)\n",
    "NEW_IMAGE_DIMS = (64, 64, 3)\n",
    "#CROP_IMAGE_DIMS = (25, 45, 153, 173)\n",
    "BATCH_SIZE = 128\n",
    "N = 6\n",
    "NUM_ATTRIBUTES = 40\n",
    "NUM_BATCHES = 1\n",
    "LATENT_DIM = 64\n",
    "TOT_IMAGES = 202599\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants needed to run in Google Colab\n",
    "PATH = './'\n",
    "IMAGES = 'img_align_celeba/'\n",
    "ATTRIBUTES = 'list_attr_celeba.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000001.jpg', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loading dataset\n",
    "def get_attributes(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    attributes = []\n",
    "    attr = []\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        if i != 0 and i != 1:\n",
    "            line = line.split()\n",
    "            #line.pop(0)\n",
    "            attributes.append(line)\n",
    "        i += 1\n",
    "    for i in range(len(attributes)):\n",
    "        for j in range(len(attributes[i])):\n",
    "            if attributes[i][j] == '-1':\n",
    "                attributes[i][j] = '0'\n",
    "    return attributes\n",
    "\n",
    "def initialize_training_set(): \n",
    "    all_attributes = get_attributes(PATH + ATTRIBUTES)\n",
    "    \n",
    "    while True:\n",
    "        chosen_info = random.sample(all_attributes, N)\n",
    "        chosen_attributes = [info[1:] for info in chosen_info]\n",
    "        chosen_images = [info[0] for info in chosen_info]\n",
    "        resized_images = []\n",
    "    \n",
    "        for ind in range(len(chosen_info)):\n",
    "            image_path = PATH + IMAGES + chosen_images[ind]\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_COLOR)[...,::-1] / 255.0\n",
    "            img = img[45:173,25:153]\n",
    "            img = cv2.resize(src=img, dsize=(64, 64))\n",
    "            resized_images.append(img)\n",
    "\n",
    "        resized_images = np.array(resized_images, dtype=np.float32)\n",
    "        chosen_attributes = np.array(chosen_attributes, dtype=np.float32)\n",
    "\n",
    "        yield [resized_images, chosen_attributes], None \n",
    "        \n",
    "def initialize_test_set():\n",
    "    all_attributes = get_attributes(PATH + ATTRIBUTES)\n",
    "    print(all_attributes)\n",
    "    info_in_batches = []\n",
    "    attributes_in_batches = []\n",
    "    images_in_batches = []\n",
    "    test_set = []\n",
    "    \n",
    "    for i in range(NUM_BATCHES):\n",
    "        chosen_info = random.sample(all_attributes, BATCH_SIZE)\n",
    "        chosen_attributes = [info[1:] for info in chosen_info]\n",
    "        chosen_images = [info[0] for info in chosen_info]\n",
    "        \n",
    "        info_in_batches.append(chosen_info)\n",
    "        attributes_in_batches.append(chosen_attributes)\n",
    "        images_in_batches.append(chosen_images)\n",
    "        \n",
    "    for i in range(NUM_BATCHES):\n",
    "        chosen_images = images_in_batches[i]\n",
    "        attributes = attributes_in_batches[i]\n",
    "        resized_images = []\n",
    "        \n",
    "        for j in range(BATCH_SIZE):\n",
    "            image_path = PATH + IMAGES + chosen_images[j]\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_COLOR)[...,::-1] / 255.0\n",
    "            img = img[45:173,25:153]\n",
    "            img = cv2.resize(src=img, dsize=(64, 64))\n",
    "            resized_images.append(img)\n",
    "\n",
    "        resized_images = np.array(resized_images, dtype=np.float32)\n",
    "        attributes = np.array(chosen_attributes, dtype=np.float32)\n",
    "\n",
    "        test_set.append([resized_images, attributes])\n",
    "    return test_set\n",
    "\n",
    "training_set = initialize_training_set()\n",
    "test_set = initialize_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_img (InputLayer)          (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   896         input_img[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 128)    73856       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 256)    295168      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 2, 2, 512)    1180160     conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 64)           131136      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_log_sigma (Dense)             (None, 64)           131136      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sampling_1 (Sampling)           [(None, 64), (None,  0           z_mean[0][0]                     \n",
      "                                                                 z_log_sigma[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104)          0           sampling_1[0][0]                 \n",
      "                                                                 labels[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,830,848\n",
      "Trainable params: 1,830,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              215040    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         295040    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 64, 64, 3)         867       \n",
      "=================================================================\n",
      "Total params: 1,783,107\n",
      "Trainable params: 1,783,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Sampling(Layer):\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def VAE():\n",
    "  #Define encoder model.\n",
    "  input_img = Input(shape = NEW_IMAGE_DIMS, name='input_img')\n",
    "  labels = Input(shape = (NUM_ATTRIBUTES,), name='labels')\n",
    "\n",
    "  x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(input_img)\n",
    "  x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "  x = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "  x = Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "  x = Conv2D(filters = 512, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu')(x)\n",
    "\n",
    "  #encode = Encode_Decode(input_img) #Done\n",
    "\n",
    "  shape_before_flattening = K.int_shape(x)[1:]\n",
    "  #shape_before_flattening = K.int_shape(encode)[1:]\n",
    "\n",
    "  x = Flatten()(x)\n",
    "\n",
    "  z_mean = Dense(LATENT_DIM, name='z_mean')(x)\n",
    "  z_log_sigma = Dense(LATENT_DIM, name='z_log_sigma')(x)\n",
    "  z = Sampling()([z_mean, z_log_sigma])\n",
    "\n",
    "  zy = Concatenate()([z, labels])\n",
    "\n",
    "  inputs_embedding = Input(shape=(LATENT_DIM + NUM_ATTRIBUTES,))\n",
    "  embedding = Dense(np.prod(shape_before_flattening))(inputs_embedding)\n",
    "  embedding = Reshape(shape_before_flattening)(embedding)\n",
    "\n",
    "  #Decoding\n",
    "  x_ = Conv2DTranspose(filters = 256, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(embedding)\n",
    "  x_ = Conv2DTranspose(filters = 128, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "  x_ = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "  x_ = Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu')(x_)\n",
    "  x_ = Conv2DTranspose(filters = 3, kernel_size = 3, strides = 2,  padding = 'same', activation = 'sigmoid')(x_)\n",
    "\n",
    "  #x_hat = Encode_Decode(embedding)\n",
    "\n",
    "  encoder = Model(inputs = [input_img, labels], outputs = zy, name=\"encoder\")\n",
    "  decoder = Model(inputs = inputs_embedding, outputs = x_, name=\"decoder\")\n",
    "\n",
    "  vae_out = decoder(encoder([input_img, labels]))\n",
    "\n",
    "  rec_loss =  np.prod(NEW_IMAGE_DIMS) * binary_crossentropy(Flatten()(input_img), Flatten()(vae_out))\n",
    "  kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "  vae_loss = K.mean(rec_loss + kl_loss)\n",
    "\n",
    "  vae = Model(inputs = [input_img, labels], outputs = vae_out, name=\"vae\")\n",
    "\n",
    "  vae.add_loss(vae_loss)\n",
    "\n",
    "  optimizer = Adam(lr=0.0005, beta_1 = 0.5)\n",
    "  vae.compile(optimizer)\n",
    "\n",
    "  return vae, encoder, decoder\n",
    "\n",
    "vae, encoder, decoder = VAE()\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "['000001.jpg', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1']\n",
      " 213/1582 [===>..........................] - ETA: 6:46 - loss: 7894.3923"
     ]
    }
   ],
   "source": [
    "vae.fit(training_set, steps_per_epoch=TOT_IMAGES//BATCH_SIZE, verbose=1, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "vae.save_weights('./vae.h5')\n",
    "decoder.save_weights('./decoder.h5')\n",
    "encoder.save_weights('./encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Frechet Inception Distance (https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/)\n",
    "\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return np.asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "    # calculate activations\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the inception v3 model\n",
    "model = InceptionV3(include_top=False, pooling='avg')#, input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_images(test_set):\n",
    "    images = []\n",
    "    predictions = []\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    for batch in test_set:\n",
    "        print('Investigating batch', index)\n",
    "        resized_images = batch[0] * 255.0\n",
    "        prediction = vae.predict(batch) * 255.0\n",
    "        \n",
    "        # Scaling images\n",
    "        original_images = scale_images(resized_images, (299, 299, 3))\n",
    "        predicted_images = scale_images(prediction, (299, 299, 3))\n",
    "        \n",
    "        # Pre-processing images\n",
    "        original_images = preprocess_input(original_images)\n",
    "        predicted_images = preprocess_input(predicted_images)\n",
    "        \n",
    "        #original_images = np.array(original_images)\n",
    "        #predicted_images = np.array(predicted_images)\n",
    "        \n",
    "        #images.append(original_images.astype(np.int32))\n",
    "        #predictions.append(predicted_images.astype(np.int32))\n",
    "        \n",
    "        images.append(original_images)\n",
    "        predictions.append(predicted_images)\n",
    "                      \n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "    \n",
    "    #images = np.array(images)\n",
    "    #predictions = np.array(predictions)\n",
    "    \n",
    "    #return images.astype(np.int32), predictions.astype(np.int32)\n",
    "    return images, predictions\n",
    "        \n",
    "images, predictions = reconstruct_images(test_set)\n",
    "\n",
    "print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating activations for each batch...\")\n",
    "img_act = np.array([]).reshape((0, 2048))\n",
    "pred_act = np.array([]).reshape((0, 2048))\n",
    "i = 1\n",
    "for batch in validation_dataset:\n",
    "    images = batch[0]\n",
    "    print(\"Batch\", i, \"- Num. samples:\", images.shape[0])\n",
    "    predictions = vae.predict(batch)\n",
    "    print('1')\n",
    "    if i == 1:\n",
    "        imgs1 = images\n",
    "        prediction1 = predictions\n",
    "    images = images * 255.0\n",
    "    predictions = predictions * 255.0\n",
    "    images1 = scale_images(images, (299, 299, 3))\n",
    "    print('2')\n",
    "    images1 = preprocess_input(images1)\n",
    "    images2 = scale_images(predictions, (299, 299, 3))\n",
    "    print('3')\n",
    "    images2 = preprocess_input(images2)\n",
    "    a = inception_model.predict(images1)\n",
    "    print('4')\n",
    "    b = inception_model.predict(images2)\n",
    "    img_act = np.concatenate((img_act, a), axis = 0)\n",
    "    print('5')\n",
    "    pred_act = np.concatenate((pred_act, b), axis = 0)\n",
    "    i += 1\n",
    "print(\"Done!\")\n",
    "#show_batch(imgs1)\n",
    "#show_batch(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.axis('off')\n",
    "    \n",
    "show_batch(images[0])\n",
    "show_batch(predictions[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, -1, 1, 1, -1]\n",
    "a = [0 if i==-1 else 1 for i in a]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
